Put your name and netid here

(1) Run the program BenchmarkForAutocomplete and copy/paste the 
results here this for #matches = 20
search	size	#match	binary	brute
20 20
	456976	20	1.0505	0.0535
a	17576	20	0.0174	0.0453
b	17576	20	0.0161	0.0331
20 20
c	17576	20	0.0453	0.0371
x	17576	20	0.0511	0.0180
y	17576	20	0.0176	0.0414
z	17576	20	0.0409	0.0279
aa	676	20	0.0003	0.0154
az	676	20	0.0003	0.0098
za	676	20	0.0003	0.0209
zz	676	20	0.0003	0.0109


(2) Run the program again for #matches = 10000, paste the results, 
and then make any conclusions about how the # matches 
effects the runtime. 

search	size	#match	binary	brute
10000 10000
	456976	10000	0.8441	0.1545
10000 10000
a	17576	10000	0.0116	0.0559
10000 10000
b	17576	10000	0.0135	0.0483
10000 10000
c	17576	10000	0.0159	0.0275
10000 10000
x	17576	10000	0.0164	0.0347
10000 10000
y	17576	10000	0.0273	0.0328
10000 10000
z	17576	10000	0.0175	0.0355
aa	676	10000	0.0003	0.0279
az	676	10000	0.0004	0.0171
za	676	10000	0.0005	0.0388
zz	676	10000	0.0004	0.0300

The number of matches increases the runtime for the brute search, changing how many
times the program must add to the list and run through the for loop, while in binary
search the number of matches actually has an inverse relationship, as it does not change
the number of times that things are added, but rather the number of times things need
to be removed, such that larger match samples results in less times running through
a for loop removing items.

(3) Copy/paste the code from BruteAutocomplete.topMatches below. 
Explain what the Big-Oh complexity of the entire loop: 
for(Term t : myTerms) {...} 
is in terms of N, the number of elements in myTerms and 
M, the number of terms that match the prefix. 
Assume that every priority-queue operation runs in O(log k) time. 
Explain your answer which should be in terms of N, M, and k.
	public List<Term> topMatches(String prefix, int k) {
		if (k < 0) {
			throw new IllegalArgumentException("Illegal value of k:"+k);
		}
		
		// maintain pq of size k
		PriorityQueue<Term> pq = new PriorityQueue<Term>(10, new Term.WeightOrder());
		for (Term t : myTerms) {
			if (!t.getWord().startsWith(prefix))
				continue;
			if (pq.size() < k) {
				pq.add(t);
			} else if (pq.peek().getWeight() < t.getWeight()) {
				pq.remove();
				pq.add(t);
			}
		}
		int numResults = Math.min(k, pq.size());
		LinkedList<Term> ret = new LinkedList<>();
		for (int i = 0; i < numResults; i++) {
			ret.addFirst(pq.remove());
		}
		return ret;
	}
}

The Big-Oh complexity would be O(N*Mlog(k)) as each priority queue operation, if going at a
complexity of log(k), would be done M times, and this would occur for every single term N found in 
the array myTerms.

(4) Explain why the last for loop in BruteAutocomplete.topMatches 
uses a LinkedList (and not an ArrayList) 
AND why the PriorityQueue uses Term.WeightOrder to get 
the top k heaviest matches -- rather than 
using Term.ReverseWeightOrder.

The last loop uses a LinkedList so that it can more easily move down the line of
nodes, and sorts by WeightOrder rather than ReverseWeightOrder so that it ensures
the nodes starting with the prefix at the highest weight order are not lost
moving down the linked list.

(5) Explain what the runtime of the 
BinarySearchAutocomplete.topMatches code that you 
implemented is by copy/pasting the code below 
and explaining your answer in terms of N, M, and k.
		for(int i = 0; i < myTerms.length; i++) {
			if(myTerms[i].getWord().substring(0, prefix.length()).equals(prefix)) {
				if(! list.contains(myTerms[i]));
				list.add(myTerms[i]);
			}
		}
		
		Collections.sort(list, new Term.ReverseWeightOrder());
		if(list.size()>k) {
			for(int i=list.size()-1; k<list.size(); i--) {
				list.remove(i);
			}
		}
		return list;
		}
		
		

